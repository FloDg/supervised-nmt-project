{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "supervised_nmt_project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "deepnote_notebook_id": "800cf0c6-d130-4999-bb07-d3c356ea4fbe",
    "deepnote_execution_queue": []
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00000-e8a231a2-42bd-4934-8039-c1c16bab7ed0",
        "id": "CUXoBs3OEzyv"
      },
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1ouZdIiuVwSlIloeygaelDhcBW5bNqk-S' width='30%' alt='liege.jpeg'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00001-4e46a5de-fa8e-439e-8966-bdf76af31a20",
        "id": "QvvaXeFiEzyy"
      },
      "source": [
        "### Preamble\n",
        "Please open this notebook with Google Colab using Google Chrome.\n",
        "If an other platform/browser is used, some code may not run correctly and some figures may not be displayed correctly.\n",
        "For instance, the figures are not displayed at all when Colab is used with Safari.\n",
        "\n",
        "All the figures and the results shown in this notebook can be found on this [git](https://github.com/FloDg/supervised-nmt-project) in the _figures_ and _results_ folders.\n",
        "\n",
        "This work is meant as a scholar project in the context of the course _Web and Text Analytics_ (INFO2049-1) of the _University of LiÃ¨ge_.\n",
        "Its authors are **De Geeter Florent**, **Nelissen Louis** and **Pirenne Thomas**.\n",
        "The sources on which this work is based are mostly cited along the notebook and are summarized in the _Sources_ section of the _Report_ part of the notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "cell_id": "00000-1eee692e-fa26-4e44-bdc3-bbf259867cb0",
        "output_cleared": false
      },
      "source": [
        "# Supervised Neural Machine Translation\n",
        "This notebook has the objective of studying the impact of various basic embeddings and the attention mechanism by Bahdanau on Neural Machine Translation. To that end, we implemented an Encoder-Decoder architecture based on LSTM cells and trained multiple models in a supervised manner. The notebook itself is made up of two parts, one dedicated to reporting our results and overviewing our methods and implementation, the other is the implementation itself.\n",
        "\n",
        "## Part 1 - Report\n",
        "This section describes our solution, presents the results we have obtained and discusses them.\n",
        "\n",
        "## Part 2 - Implementation\n",
        "This section contains the actual implementation of the project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00003-77f11e39-4f36-4bfd-9415-44f96c1db8b1",
        "output_cleared": false,
        "id": "ntnEN-KcEzy2"
      },
      "source": [
        "# Part 1 - Report\n",
        "1. Implementation\n",
        "    1. Overview\n",
        "        1. Dataset\n",
        "        2. Embeddings\n",
        "        3. Architecture\n",
        "        4. Attention\n",
        "        5. Training\n",
        "    2. Problems encountered\n",
        "    3. Considered solutions\n",
        "2. Results\n",
        "    1. Word2Vec\n",
        "    2. GloVe\n",
        "    3. fastText\n",
        "    4. Comparison\n",
        "    5. Attention\n",
        "3. Discussion\n",
        "4. Improvements\n",
        "5. Sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00003-3dc16bd8-e484-480d-a1b2-ab9531a37c95",
        "output_cleared": false,
        "id": "uBgtEeyoEzy3"
      },
      "source": [
        "## Implementation\n",
        "This section starts with an overview of our implementation choices, more specifically the way we exported, created and used embeddings, the way we designed our architecture, the basic principles of the attention mechanism applied to MT and the way we trained the models. The section then presents the main problems we encountered and finishes by describing the solutions we found and considered for those problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00004-19c770a1-e4d2-4e3d-ace5-979ccb970b3d",
        "output_cleared": false,
        "id": "s6Awn6GPEzy5"
      },
      "source": [
        "### Overview\n",
        "\n",
        "The base of our implementation was largely inspired by the Tensorflow tutorial [_Neural Machine Translation with attention_](https://www.tensorflow.org/tutorials/text/nmt_with_attention) cited at the end of the **Report** section. We started off by removing the attention mechanism from it and replacing the GRU by LSTM cells. We also removed the embedding layer used in the tutorial and replaced it by pretrained and custom embeddings. We chose to do this for the _Word2Vec_, _GloVe_ and _FastText_ embeddings.\n",
        "\n",
        "Later on, we chose to reimplement the attention mechanism shown in that same tutorial and then train models with it to judge its usefulness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00005-7635635c-019b-4f99-8b9d-f231250d8bb7",
        "output_cleared": false,
        "id": "lj5XdVrREzy6"
      },
      "source": [
        "#### Dataset\n",
        "\n",
        "In the Tensorflow tutorial, the authors use a Spanish-English dataset found on _manythings.org_. \n",
        "We decided to train our models to translate sentences from English to French and found an English-French parallel dataset on this very same website.\n",
        "This dataset consists in about 170,000 bilingual sentences.\n",
        "The decision to translate from English to French is motivated by the fact that French is more complex than English. \n",
        "Indeed, this complexity is such that there might be on average more French interpretations for the same English sentences. \n",
        "Since the task of translating from a language to another consists in interpreting a source sentence and outputting a single sentence in the target language of similar interpretation, it seemed like the task of translating from English to French would be slightly easier than translating from French to English.\n",
        "\n",
        "Additionally, we chose to only train a model to translate in one direction (i.e. FR -> EN) but training it in the other direction should not require any modification of the architecture. In fact, it should only require to swap the _source_ and _target_ sentences in the dataset before training as the tasks are similar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00005-1dc86279-a607-4e55-8a01-cd4f7ddc7098",
        "output_cleared": false,
        "id": "U90yg21vEzy7"
      },
      "source": [
        "#### Embeddings\n",
        "We considered the 3 following word embedding techniques: [_Word2Vec_](https://arxiv.org/abs/1301.3781), [_GloVe_](https://nlp.stanford.edu/projects/glove/) and [_FastText_](https://fasttext.cc/).\n",
        "For each of those, we had two options: either use **pretrained** word embeddings or train **custom** embeddings on our own dataset. \n",
        "<!-- For the pretrained embeddings we use the [gensim downloader API](https://radimrehurek.com/gensim/downloader.html) to import one of the word embedding models [they propose](https://github.com/RaRe-Technologies/gensim-data). -->\n",
        "We will now go over each in detail:\n",
        "\n",
        "##### Pretrained Word2Vec \n",
        "- EN ðŸ‡¬ðŸ‡§ - We chose one of the pretrained models provided by the [gensim downloader API](https://radimrehurek.com/gensim/downloader.html). \n",
        "In particular we used a model trained on [Google News](https://github.com/RaRe-Technologies/gensim-data) articles using Word2Vec CBOW. \n",
        "This model learnt over 3 million words and phrases, modelled in a vector of _300_ dimensions.\n",
        "\n",
        "- FR ðŸ‡«ðŸ‡· - We sourced French Word2Vec embeddings from [Jean-Phillipe Fauconnier's website](https://fauconnier.github.io/) as binary, loaded in Keyed Vectors. \n",
        "These word embeddings are trained on [frWaC](https://wacky.sslmit.unibo.it/doku.php?id=corpora#french), a dataset of 1.6 billion words sourced from a web crawl of the **.fr** domain. \n",
        "The embeddings are modelled in _200_ dim and use the Continuous Bag of Words method.\n",
        "From the different models available, we tried to pick the closest equivalent to the english language embdedder we use.\n",
        "\n",
        "##### Custom Word2Vec\n",
        "- EN ðŸ‡¬ðŸ‡§ & FR ðŸ‡«ðŸ‡· - We trained both word embeddings on our dataset using [gensim's Word2Vec model](https://radimrehurek.com/gensim/models/word2vec.html). \n",
        "We mapped these embeddings in _100_ dimensions, a number we chose arbitrarily. \n",
        "Since our dataset is considerably smaller than the datasets commonly used to pretrain embeddings, we chose this dimension to be smaller as well which somewhat motivates the value _100_.\n",
        "We did try to take a number that is neither too high nor too low as our dataset is limited in size.\n",
        "<!-- Custom w2v -> With gensim models, train on our data (format?) gives a dictionary in the form of KeyedVectors. Training takes a certain amount of time ? -->\n",
        "\n",
        "##### Pretrained GloVe\n",
        "- EN ðŸ‡¬ðŸ‡§ - Again, we decided to one of the pretrained models provided by the [gensim downloader API](https://radimrehurek.com/gensim/downloader.html). \n",
        "In this case we used a [model](https://github.com/RaRe-Technologies/gensim-data) trained on a combination of [Wikipedia text crawl from 2014](https://dumps.wikimedia.org/enwiki/20140102/) and [gigaword](https://catalog.ldc.upenn.edu/LDC2011T07), a dataset made from aggregation of news articles in English. \n",
        "This model learnt a vocabulary of over 400,000 words, modelled in a vector of _300_ dimensions (there were several other options available but we chose to use 300 to keep as much consistency as possible with Word2Vec).\n",
        "- FR ðŸ‡«ðŸ‡· - We did not find any pretrained GloVe model for the French language. We therefore use a custom model, as detailed below.\n",
        "\n",
        "\n",
        "##### Custom GloVe \n",
        "<!-- -> With glove_python package, train model, then adjust this model to work in the same format as w2v (for genericity in our code) -->\n",
        "- EN ðŸ‡¬ðŸ‡§ & FR ðŸ‡«ðŸ‡· - We used the `glove_python` package to generate and train our custom GloVe model. \n",
        "To that end we followed this [Medium tutorial on GloVe](https://medium.com/analytics-vidhya/word-vectorization-using-glove-76919685ee0b) and trained both French and English embeddings on our dataset. \n",
        "Similarly as for the other custom embeddings, we chose to dimension the vectors to _100_ in order to make the models as comparable as possible.\n",
        "\n",
        "##### Pretrained fastText \n",
        "<!-- -> from gensim downloader api trained on wikinews (size?). -->\n",
        "- EN ðŸ‡¬ðŸ‡§ - Again, we decided to one of the pretrained models provided by the [gensim downloader API](https://radimrehurek.com/gensim/downloader.html). \n",
        "In this case we used a [model](https://github.com/RaRe-Technologies/gensim-data) trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset.\n",
        "The embeddings are modelled in a vector of _300_ dimensions. \n",
        "An important note is that we get a dictionary and not the actual model, meaning with this model we cannot find embeddings for out-of-vocabulary words.\n",
        "- FR ðŸ‡«ðŸ‡· - We did not find any pretrained fastText model for the French language. We therefore use a custom model, as detailed below.\n",
        "\n",
        "##### Custom fastText \n",
        "<!-- -> With gensim models, train on our data too. Gives dictionary in the form of KeyedVectors too. -->\n",
        "- EN ðŸ‡¬ðŸ‡§ & FR ðŸ‡«ðŸ‡· - We trained both word embeddings on our dataset using [gensim's Word2Vec model](https://radimrehurek.com/gensim/models/word2vec.html).\n",
        "As with the pretrained model, we do get a dictionary and not an actual model.\n",
        "We mapped these embeddings in _100_ dimensions, to keep consistency with the other custom models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00006-8667406e-ba7b-4dad-8bda-50a1753ef82d",
        "output_cleared": false,
        "id": "RROnpWAFEzy8"
      },
      "source": [
        "#### Architecture\n",
        "We chose to use a traditional Encoder - Decoder Recural Neural Network (RNN) architecture, inspired by Google's [seq2seq](https://google.github.io/seq2seq/) architecture. This architecture consists of two RNN networks. \n",
        "The first network (the Encoder) encodes a source sentence of variable length into a state with a fixed shape. \n",
        "The second network (the Decoder) takes this state and decodes it into a variable-length sequence, in our case a sentence in our target language.\n",
        "Encoder - Decoder RNNs were one of the first architecure used in Neural Machine Translation and are still today a very solid option.\n",
        "\n",
        "For the choice of which type of RNN we would use, we chose to work with LSTM. The tutorial which we used as foundation for out propject used a GRU, but we picked LSTM instead because it decouples the long term and short term memory which we see as a desirable property.\n",
        "In addition, LSTM seem more prevalent in literature, and for many application, they are juged to be at least as good as the GRU. However, this choice remains vastly arbitrary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00007-92f033a9-1b7b-48c1-b011-0e6b1877640b",
        "output_cleared": false,
        "id": "O8QHgIr6Ezy9"
      },
      "source": [
        "#### Attention Mechanism\n",
        "An early problem of RNNs in NMT was that they tended to have a hard time working with very long input sentences as a single vector. \n",
        "A common solution to this is to use an [attention mechanism](https://arxiv.org/pdf/1409.0473.pdf). \n",
        "The reason why the classical Encoder-Decoder RNN scheme has trouble with long input sequences is that the longer it is, the more difficult it is for the encoder to provide an output state that characterize equally the first words of the sequence and the last ones. Since the decoder typically takes as input the output of the encoder i.e. its last hidden state, it comes that with long sequences, this output is somewhat less influenced by the first words of the sequence than the last. This asymetry makes it difficult for the decoder to consider effectively first words of the sequence. The idea that comes naturally to solve this problem is to provide all hidden states of the encoder to the decoder instead of just the last one. However, this would result in providing much superfluous information to the decoder, thus making it difficult to extract the actually useful information. This idea is the foundation of the attention mechanism.\n",
        "Indeed, this mechanism is trained to allow the decoder to focus its attention on the relevant parts of the encoder's hidden states. I.e. for a word to output by the decoder, the attention mechanism will allow it to focus on the hidden states that the encoder outputted for the words that are relevant to the translation of the former word to output.\n",
        "A good attention mechanism will therefore let the Decoder focus on the more important parts of a source sentence. \n",
        "This has been proved to increase the performance of Encoder-Decoder RNNs but also other NMT architectures such as CNNs. \n",
        "Modern NLP models based on Transforms (such as BERT or GPT) directly incorporate attention in their design.\n",
        "\n",
        "In practice, we used the Bahdanau attention mechanism implemented in the same [tensor flow tutorial](https://www.tensorflow.org/tutorials/text/nmt_with_attention) that we have been following for our implementation.\n",
        "In terms of implementation, the attention mechanism can be seen as an additional layer in the decoder before the LSTM cell. It takes as input all of the hidden states from the Encoder and obviously the previous hidden state from the decoder.\n",
        "Where it becomes open for debate is whether that \"hidden state\" is actually the proper hidden state of the LSTM or its cell state or both concatenated.\n",
        "Our understanding of the attention mechanism makes us believe that using the cell state, which is the long term memory of the LSTM unit, would only serve to noise the state the attention mechanism tries to extract relevant information from.\n",
        "Indeed, as the attention mechanism is supposed to extract information characteristic of the specific word in the sequence the hidden state is from and since the cell state only carries little information on said word (being the long term memory), we believed it wiser to provide the hidden state (short term memory) of the LSTM to the attention mechanism.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00007-2b58659d-5f67-432b-9306-80dec7ba5828",
        "output_cleared": false,
        "id": "l0HHvpS3Ezy_"
      },
      "source": [
        "#### Training\n",
        "For training, we used the dataset described in section **Dataset** and split it into training and validation sets.\n",
        "For the actual training, we used the whole dataset which is obtained by setting the parameter _num_examples_ to **None** in the Constants section of the implementation.\n",
        "Of the _num_examples_ sentences chosen from the dataset, we randomly (but with a fixed random seed) chose 10% of them for validation and dedicated the rest to training.\n",
        "\n",
        "As GPU running time is limited on colab, we chose to train each of our models for 10 epochs.\n",
        "One epoch of training on the whole dataset lasted between 30 and 65 minutes depending on the colab sessions.\n",
        "This makes training one model last between 5 and 11 hours."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00008-c26261ec-873f-4f5d-9ab7-5423435e549c",
        "output_cleared": false,
        "id": "XHKVuEHwEzzB"
      },
      "source": [
        "### Problems Encountered & Solutions\n",
        "\n",
        "In this section we will go over different problems and challenges we have faced during this project and the solutions we thought of and implemented in the different cases.\n",
        "\n",
        "#### Embedding problems\n",
        "- **Punctuation** \n",
        "    - **Problem**: The pretrained Word2Vec embeddings we found did not include punctuation.\n",
        "    However, for a translation task, punctuation has meaning and we thus needed a way to represent it to feed to the networks.\n",
        "\n",
        "    - **Solution**: To solve this problem we simply chose to remove the punctuation from the dataset when  _w2v_pretrained(_fr/_en)_ is used. \n",
        "    We could have fine-tuned the gensim embedding model with our dataset in order for it to learn an embedding for punctuation but we believed that the first fix would be sufficient for our purposes.\n",
        "    Additionally, we thought it would not be useful to implement \n",
        "    this solution as we meant to train our own custom embeddings from scratch in subsequent tests. In order \n",
        "    to have comparison points between the custom embeddings and the pretrained ones, it seemed wise not to \n",
        "    fine tune the pretrained embeddings on the dataset the custom ones are trained on. \n",
        "\n",
        "- **Out-Of-Vocabulary**\n",
        "    - **Problem**: All pretrained embeddings faced the out-of-vocabulary issue for different words.\n",
        "    The punctuation issue mentioned above is actually a specific OOV issue.\n",
        "    It is an issue because these words cannot be embedded and if the embedder is asked on such _unknown_ words, the program will crash.\n",
        "\n",
        "    - **Solution**: To solve this problem we simply replaced each out-of-vocabulary word by a handmade embedding \n",
        "    containing only zeros.\n",
        "\n",
        "    Another option would be, as mentioned for the punctuation problem, to fine tune \n",
        "    each embedding model with our custom vocabulary. However, this solution is more easily applied for \n",
        "    word2vec and can actually become complex for the others as the models we import from the gensim \n",
        "    downloader are actually vectors and not trainable models. \n",
        "    Moreover, the comparison argument provided in the punctuation solution stays valid for this as well.\n",
        "\n",
        "    A third possible solution could have been to remove the sentences containing unknown words from the dataset \n",
        "    and train the network on a reduced dataset. However this solution limits the dataset used and thus the \n",
        "    potential performance of the network. \n",
        "    Moreover, it is especially inadpated to the punctuation problem because it would mean removing all sentences \n",
        "    from the dataset.\n",
        "\n",
        "- **fastText**\n",
        "    - **Problem**: As stated previously, we don't have an actual proper fastText model.\n",
        "    Theoretically, fastText embedding should enable us to embed words that were not present in the \n",
        "    dataset used to train the embedder (therefore solving the out-of-vocabulary problem). \n",
        "    However, the pretrained embedder obtained with the gensim downloader only provides a dictionary mapping words (keyed vector) to embeddings. \n",
        "    This means it can't actually give an embeding for words it has not encountered.\n",
        "    <!-- though it's not really a problem for the custom embedder as all the test phrases are in the embedder's vocabulary.-->\n",
        "\n",
        "    - **Solution**: We could have searched for a way to export the model rather than the dictionary.\n",
        "    Nevertheless, for the sake of simplicity, we chose to treat these OOV words the same way as for the other embeddings, as described in the previous paragraph.\n",
        "    Note that regarding the custom fastText embedder, even though we manipulate the model itself, we did not implement a way to embed unkown words even though it might have been possible.\n",
        "    The reason for that is to avoid complicating uselessly our implementation.\n",
        "    Indeed, since the model is trained on our dataset, all of the words it contains will be known and we will thus never face an OOV problem while validating.\n",
        "    The OOV problem could still appear if translating sentences from outside the dataset but this special scenario did not justify that we spend time implementing the above-mentioned solution.\n",
        "\n",
        "\n",
        "- **French Embeddings**\n",
        "    - **Problem**: We could not find suitable pretrained French embeddings for GloVe nor fastText.\n",
        "    This is a problem in that French embeddings are mandatory for the decoder to produce an interpretable output.\n",
        "    Using one of the French embedders that we found for embeddings other than GloVe or fastText is a problem in that it would undermine the validity of our comparisons of the embeddings' performances.\n",
        "\n",
        "    - **Solution**: We chose to replace those pretrained embeddings by custom embeddings of the same type.\n",
        "    This makes comparisons of the results of pretrained embeddings with custom ones less reliable but as it was only embeddings in our target language that were replaced, it seemed like the conclusions would still carry some weight.\n",
        "\n",
        "- **Heterogeneity of pretrained embeddings** \n",
        "    - **Problem**: The pretrained embeddings we found are not uniformly trained: some have different dimensions than the others and all have been trained on different corpora and even on corpora with different scales.\n",
        "    This is a problem only because it slightly undermines the validity of our comparisons of the embeddings' performances.\n",
        "    <!-- Since the pretrained embeddings were not uniformly trained \n",
        "    i.e. some were embedded into different vector dimensions and some were trained on corpora of different scales, \n",
        "    the reliability of the conclusions that we could make while comparing the different embedding schemes is \n",
        "    diminished as well.  -->\n",
        "    - **Solution**: This motivates further the use of custom embeddings, which are trained on the exact same\n",
        "    corpora (we used a fixed random seed to ensure that it is the case) and to the same vector dimensions.\n",
        "\n",
        "\n",
        "#### Other problems\n",
        "\n",
        "- **RAM saturation**\n",
        "    - **Problem**: This problem is rather self-explanatory.\n",
        "    The way we first implemented our training and more specifically the place in the pipeline where we embedded the dataset caused the 12GB of RAM available on Colab to be saturated.\n",
        "    The reason why this is a problem is that Colab resets the machine when this happens, thus losing the local variables of the environment.\n",
        "\n",
        "    - **Solution**: At first we used to embed all of the sentences of the dataset and then we'd pass the huge vector to the \n",
        "    encoder and decoder for it to learn. However, this requires to load the whole embedded dataset as a block \n",
        "    into the RAM and it saturated it on _colab_. As a result, the solution was pretty straight forward i.e. embedding the dataset one batch at \n",
        "    a time directly in the encoder in order to allow the OS to evict and bring back parts of the former huge \n",
        "    vector from and to RAM, thus making the computation slightly slower but preventing the virtual environment \n",
        "    of colab from crashing.\n",
        "\n",
        "- **Colab Shutting off**\n",
        "    - **Problem**: To train our algorithms, we have chosen to work with Google Colab.\n",
        "    Google Colab generously allocates us a certain amount of GPUs to do our work. \n",
        "    Unfortunately it stops working when the user is not active after a certain amount of time, which is rather impractical considering that a training lasts at least 5 hours.\n",
        "\n",
        "    - **Solution**: Therefore, we use a javascript code in our browser to make it periodically open a certain window to make Colab believe we're still active. The code is the following:\n",
        "\n",
        "        function ClickConnect()\n",
        "        {\n",
        "            console.log('Working');\n",
        "            document.querySelector('colab-connect-button').shadowRoot.getElementById('connect').click();\n",
        "        }\n",
        "        x = setInterval(ClickConnect, 60000);\n",
        "    \n",
        "    It works pretty well.\n",
        "    Big thanks to [Shivam Rawat](https://medium.com/@shivamrawat_756/how-to-prevent-google-colab-from-disconnecting-717b88a128c0) for the fix!\n",
        "\n",
        "\n",
        "- **Length of Training Time**\n",
        "    - **Problem**: Training a Neural Machine Translation model takes a lot of data and training on a lot of data takes a lot of time.\n",
        "    In our case, the time it takes to train our models is fundamental as we both have a deadline to submit our work and have training length restrictions on Colab (which is limited to 12 hours).\n",
        "\n",
        "    - **Solution**: To make this problem tracktable, we trained our models for 10 epochs only. Each training took about 5 hours when we managed to connect to one of Colab's better GPUs \n",
        "    and due to Colab timeouts, we had to restart many training sessions. This project serves as proof of concept \n",
        "    and if the objective was to actually train a model for a day-to-day application, we would pick a training\n",
        "    strategy among those tried in this probject and use it to train a definitive model for more epochs and \n",
        "    possibly on more data. A discussion on which one we'd chose is given in the section 'Discussion'.\n",
        "\n",
        "- **Custom embeddings checkpoints**\n",
        "    - **Problem**: To restore a model's state to after being trained without having to retrain it, we use tensorflow checkpoints.\n",
        "    However, these checkpoints work weirdly with custom embeddings i.e. the network cannot translate anything, even what it could translate right after training.\n",
        "    There seems to be a random factor in the training of the embeddings, which makes them different each time they are trained.\n",
        "    Said-random factor is either the seed which was documented to be fixed but may not actually be or it is the fact that we use multiple workers which reorders the tasks of each worker based on hardware factors such as processor scheduling.\n",
        "    The consequence is that the training of the network is done on certain embeddings and when restoring the checkpoint of the network, the embeddings used to embed the words are different.\n",
        "    As a result, the network cannot recognize the input words and in these circumstances, it obviously cannot translate them.\n",
        "\n",
        "    - **Solution**: If it is just a seed problem, it is possible to fix one, however this solution did not seem to work as easily as that, it is thus likely that this is not the problem. \n",
        "    If the problem is the reordering of worker's tasks, then the solution is to set the number of workers to 1. \n",
        "    Nevertheless, this solution did not seem to work as straightforwardly as that either.\n",
        "    We thus decided to store the custom embeddings model in a file, in order to be able to reload it later.\n",
        "    Consequently, when it is asked to create a custom embeddings model, we first search if a file with the correct name\n",
        "    exists, and if it is the case we load it, otherwise we train a new model and save it in the file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00010-b4a9664a-0131-4726-9f3b-9402d26f7154",
        "output_cleared": false,
        "id": "SJYWEo5UEzzB"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00011-fcab19f5-d28d-4f12-8bee-22057b59ee98",
        "output_cleared": false,
        "id": "ocrwTzZJEzzC"
      },
      "source": [
        "This section is meant to illustrate and discuss the results obtained by each of the models we trained.\n",
        "We trained models using _Word2Vec_, _GloVe_ and _FastText_ embeddings.\n",
        "For each embedding, we trained 3 models based on:\n",
        "- pretrained embeddings\n",
        "- our custom embeddings\n",
        "- our custom embeddings and using the attention mechanism\n",
        "\n",
        "The section first shows each embedding's loss and score results to compare the pretrained with the custom embeddings.\n",
        "It then compares the scores of the different embeddings without attention mechanism.\n",
        "Only then does it compare the custom embeddings with and without the attention mechanism.\n",
        "\n",
        "It is interesting to note that to evaluate the performance of a model, we computed the _BLEU_ scores (with a smoothing function) of each validation sentence and displayed the average and standard deviations of those scores on the whole validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00011-8436c6e1-093d-4cf7-999f-4b93a353f252",
        "output_cleared": false,
        "id": "TDg_MD0REzzD"
      },
      "source": [
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00015-783a52bf-bbe8-4327-b695-08191dcc6204",
        "output_cleared": false,
        "id": "bM8YFdQqEzzE"
      },
      "source": [
        "**Losses** - We can probably see that the losses have the expected negative exponential curve converging to about 0.05.\n",
        "Both loss curves converge rather similarly.\n",
        "The custom embeddings seem to start with a lower loss but end up decreasing less than the pretrained embeddings-based model.\n",
        "This slight difference is inconsequential however since the difference between the two curves is within the standard deviation of each.\n",
        "The conclusion in terms of loss is that it is very similar for both models and that both learned from the dataset.\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1HQBw6XDjR7kGiXMH3ktb1cG6qTzCe82P' width='70%' alt='w2v.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00015-73ae4cb4-4259-400e-9e24-17069e4267d1",
        "id": "3-xvWKt3EzzE"
      },
      "source": [
        "**Scores** - The first thing to notice is that the standard deviation of the BLEU scores is of the same scale as the average score itself, which is very high.\n",
        "This means that the scores are highly sensitive to the sentence they score. As a result, the interpretations of the average scores that we make are to be taken with a grain of salt.\n",
        "The only thing we can be sure of is that it means that our translator can translate some sentences very accuretely (most likely those similar to the training sentences) and some terribly.\n",
        "This tends to show signs of overfitting the training sentences.\n",
        "\n",
        "It can be observed that the average BLEU score is better for the pretrained embedding-based model than for the custom one.\n",
        "One justification of this is that the pretrained embeddings are more representative of the words than the custom ones which may be due to the fact that it was trained on a lot more data and very likely for longer.\n",
        "Another justification is, once more, that the pretrained embeddings have a higher dimension than the custom ones.\n",
        "\n",
        "| Embedding           | BLEU score         | Standard deviation |\n",
        "|:--------------------|:-------------------|:-------------------|\n",
        "| Word2Vec pretrained | 0.342048           | 0.310822           |\n",
        "| Word2Vec custom     | 0.309999           | 0.294159           |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00012-85385ea9-fccf-4796-80c9-fb167c8e0f13",
        "output_cleared": false,
        "id": "VWT_NyggEzzG"
      },
      "source": [
        "### GloVe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00017-8970b1c7-0ef8-4e2c-a9a7-e36ee973c452",
        "output_cleared": false,
        "id": "rCIgiO2nEzzG"
      },
      "source": [
        "**Losses** - The first thing to notice is that the loss of the custom embeddings-based model starts and stays significantly lower than the custom one.\n",
        "If compared to the Word2Vec loss curves, it seems like the pretrained GloVe loss curve goes down as much as the Word2Vec curves while the custom GloVe curve reaches twice their loss after 10 epochs i.e. about 0.1.\n",
        "In a word, it seems like the custom GloVe model had troubles learning from the same dataset as the other models.\n",
        "The reason behind this behavior may be that we misparametrized the learning of the GloVe embeddings themselves.\n",
        "For instance, we used a learning rate of 0.05 and trained the embeddings for 30 epochs on our dataset.\n",
        "These are standard values used in the [tutorial](https://medium.com/analytics-vidhya/word-vectorization-using-glove-76919685ee0b) we followed and they are not motivated in any way so they may be misguided.\n",
        "Moreover, we did not extract the evolution of the loss with respect to the epochs, we thus have no way of knowing whether it actually converged or not.\n",
        "\n",
        "The impact embeddings may have on the loss of the translation task's training is that if the embeddings do not characterize clearly each word, it may be more difficult for the network to interpret them and thus to learn to translate them.\n",
        "In a word, one could say that the network possibly confuses words more due to uncharacteristic embeddings and it would thus have more trouble learning to translate them.\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1D5-IKXv8K1v507Cjp6qsKUck8Z9D5ViU' width='70%' alt='glove.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00018-683a1f79-acaa-460d-82f4-96ee9fe3e3b2",
        "id": "dsnD4Hf_EzzH"
      },
      "source": [
        "**Scores** - The same observation of the standard deviation can be done for GloVe embedding-based models than for Word2Vec i.e. that they are very high.\n",
        "\n",
        "The average scores are a lot better for the pretrained embedding-based model than for the custom one.\n",
        "In this case, this was expected seeing as the loss did not decrease as much.\n",
        "The loss curves showed that the model had troubles learning from the dataset, it is thus not surprising that the resulting model translates worse.\n",
        "\n",
        "| Embedding           | BLEU score         | Standard deviation |\n",
        "|:--------------------|:-------------------|:-------------------|\n",
        "| GloVe    pretrained | 0.355462           | 0.309589           |\n",
        "| GloVe    custom     | 0.240517           | 0.267595           |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00016-3af82415-c15e-42db-aa3b-f42754e7a556",
        "output_cleared": false,
        "id": "nLrAH63dEzzH"
      },
      "source": [
        "### FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00019-b79fa45b-c294-4263-8ec1-e4c999ab1cb0",
        "output_cleared": false,
        "id": "7IGcds50EzzI"
      },
      "source": [
        "**Losses** - The loss curves of the models based on fastText embeddings have shapes that are very similar to those of Word2Vec based models.\n",
        "A slight difference might be that the custom and pretrained curves seem to join rather than cross but given more epochs it is likely that they would have crossed the same way as the Word2Vec curves.\n",
        "\n",
        "Apart from the custom GloVe curve which may be a sort of mishandling, both the Word2Vec and the FastText custom curves start off with a lower loss than th epretrained ones.\n",
        "A hypothesis as to what it could be due is that custom embeddings have a lower dimension (100) than pretrained embeddings (~300).\n",
        "Indeed, an smaller representation of each word makes it easier to learn to interpret them early on but is more limited in terms of nuances.\n",
        "The tendency described in the previous sentence is exactly the one we find in the plots i.e. a lower custom loss early on which stabilizes faster (and thus higher) than the pretrained models' losses.\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1sv3SnKVFFDhzjT_a49BLQCZXIy2kDSmD' width='70%' alt='ft.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00021-7ca3abe7-1a8b-4c12-aaba-3ca20f01957b",
        "id": "AFwqcBi3EzzI"
      },
      "source": [
        "**Scores** - The standard deviation was very high for Word2Vec and GloVe and FastText is no exception, the standard deviation is almost as high as the average score itself.\n",
        "\n",
        "What changes from the others embeddings is that it seems like the custom FastText's average score is hardly smaller than pretrained FastText's.\n",
        "Relating this to the loss curve can be done by remebering that when the training stopped, both loss curves were just crossing each other.\n",
        "The loss curves (based on the training set) and the scores (based on the validation set) seem to be strongly correlated which leads to believe that the models can somewhat generalize their learning from the learning set to the validation set.\n",
        "Indeed, the final loss on the training set is almost the same for both models and so is the average score on the validation set.\n",
        "The hypothesis that training both models for more epochs would have the loss curves cross can be extended to the average scores:\n",
        "It is likely that training them for more epochs would increase the difference in score between both models to yield a significantly better pretrained than custom model.\n",
        "\n",
        "| Embedding           | BLEU score         | Standard deviation |\n",
        "|:--------------------|:-------------------|:-------------------|\n",
        "| Fasttext pretrained | 0.312947           | 0.296650           |\n",
        "| Fasttext custom     | 0.306072           | 0.292292           |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00014-309c9e36-5bd9-44ee-8e6d-fd765c07da05",
        "output_cleared": false,
        "id": "K5PxE3gKEzzJ"
      },
      "source": [
        "### Comparison\n",
        "It seems like overall, the pretrained embedding-based models tend to have better scores than the custom ones.\n",
        "Regardless, after 10 epochs, the custom models seem to remain competitive.\n",
        "However, the loss curves let us expect that given more epochs, the pretrained-based models could significantly outdo their custom counterparts.\n",
        "The justification of this statement is that the pretrained embeddings have a higher dimension than the custom ones and they have thus more potential for interpretation.\n",
        "\n",
        "The rest of this section serves to compare the performances of the embedding methods with respect to one another."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00021-612a8ae9-75d4-44fa-888e-73c2cb00964a",
        "output_cleared": false,
        "id": "upSZ-vBoEzzJ"
      },
      "source": [
        "**Pretrained Losses** - The pretrained losses are more similar than they each were with respect to their corresponding custom losses.\n",
        "This further corroborates the statement that there is a correlation between the custom-or-not factor of the embeddings and the models' ability to learn to translate.\n",
        "As a reminder, we hypothesized that it was mainly due to the dimension of the embedding which is significantly lower for custom than for pretrained embeddings.\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1QQCqFZmD6e1j4ko_6FEt5BzTCwHHwzmp' width='70%' alt='pretrained.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00022-560306cd-0943-4436-b342-9f638fe74ff0",
        "output_cleared": false,
        "id": "zdYKz62AEzzK"
      },
      "source": [
        "**Custom Losses** - The custom loss curves have the same shape but as mentioned, the custom GloVe embeddings yielded a significantly worse loss curve than the others.\n",
        "As mentioned, we believe that this is a result of our inexperience in training embeddings rather than an actual weakness of GloVe itself or the GloVe training framework we used.\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1zPvVKfNI1tMcEl92B5ywPV_4NDVpSaur' width='70%' alt='custom.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00020-3e6d00fe-dea8-49f1-bcd2-41b6c18a5d9c",
        "output_cleared": false,
        "id": "Hzh2gyPTEzzK"
      },
      "source": [
        "**Scores** - The performance of pretrained with respect to custom embeddings have been discussed before but can still be observed in this recapitulative table.\n",
        "\n",
        "What is newly highlighted by this table however, is the fact that among the pretrained embedding-based models, Word2Vec and GloVe seem to perform better than FastText.\n",
        "Once more, this can only be said for our specific implementation and dataset as the standard deviations are so high that our results can hardly be generalized to the embedding schemes themselves.\n",
        "Nevertheless, in our case, it seems like GloVe pretrained performs best and with the lowest standard deviation with respect to its average value.\n",
        "\n",
        "| Embedding           | BLEU score         | Standard deviation |\n",
        "|:--------------------|:-------------------|:-------------------|\n",
        "| Word2Vec pretrained | 0.342048           | 0.310822           |\n",
        "| Word2Vec custom     | 0.309999           | 0.294159           |\n",
        "| GloVe    pretrained | 0.355462           | 0.309589           |\n",
        "| GloVe    custom     | 0.240517           | 0.267595           |\n",
        "| Fasttext pretrained | 0.312947           | 0.296650           |\n",
        "| Fasttext custom     | 0.306072           | 0.292292           |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00023-fcb41203-083f-46ec-bb32-536db80be886",
        "output_cleared": false,
        "id": "hxLA5z94EzzK"
      },
      "source": [
        "### Attention\n",
        "In this section, we compare the performances of our models based on each of our custom embeddings with and without the use of Bahdanau's attention mechanism.\n",
        "To that end, we both take a look at the evolution of the loss as well as the BLEU score they obtain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00027-22223755-2c9a-4907-924e-fb4d19a19560",
        "id": "O2xPgpRcEzzL"
      },
      "source": [
        "**Losses** - The Word2Vec curves being very similar, we can infer that the model learns as easily to translate the training dataset whether the attention mechanism is used or not.\n",
        "The same can be said of the FastText curves, however, the GloVe loss curves are dissimilar.\n",
        "It was noticed previously that the GloVe custom loss curve was already higher than all of the other loss curves, which lead us to believe that it was due to a misparametrization of the embedding's training on our part.\n",
        "Since the GloVe model with attention is also based on these very same embeddings, it was expected that it would also have trouble learning.\n",
        "However we can notice that the attention mechanism seems to have helped the model learn as it brought the loss curve back to a similar trend than the other models.\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=11kRLMFz6F6Zd49kuERtOQx1Hkk0jVdFa' width='70%' alt='w2v_attention.png'>\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=12yfGOLvSL9DxhQNiNPWNbB3_QWixdKnU' width='70%' alt='glove_attention.png'>\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1wvwD1nVmyoo1pNLAc3UneygZx73lGzi1' width='70%' alt='ft_attention.png'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00027-5d9cce21-751c-4b8f-b6f5-e3d21336af0f",
        "id": "7dtuMKbmEzzL"
      },
      "source": [
        "**Scores** - It can be seen that regardless of the type of embedding, using the attention mechanism systematically improves the BLEU score.\n",
        "We can safely assume that the attention mechanism is indeed very useful to increase the performance of the models.\n",
        "Additionally, it improves the BLEU score at barely any cost in computation. Indeed, each epoch took only slightly longer with the attention mechanism than without and the loss evolution is almost identical, which shows that the attention mechanism does not make it more difficult for the network to learn.\n",
        "\n",
        "An interesting observation is that, regardless of the seemingly bad base that the custom GloVe embeddings represents with a mere 24 average score, the attention mechanism on top of it managed to make its performance competitive with th eother embeddings.\n",
        "It made the average score go up by 0.10 which is more than the 0.06 increase that it brought to Word2Vec and FastText.\n",
        "\n",
        "One additional thing to notice is that the attention mechanism increased the average score of each model considerably while increasing less than proportionately the standard deviation of said scores.\n",
        "They remain very high regardless but it is still an improvement.\n",
        "\n",
        "| Embedding           | BLEU score         | Standard deviation |\n",
        "|:--------------------|:-------------------|:-------------------|\n",
        "| Word2Vec            | 0.309999           | 0.294159           |\n",
        "| Word2Vec attention  | 0.357615           | 0.312432           |\n",
        "| GloVe               | 0.240517           | 0.267595           |\n",
        "| GloVe    attention  | 0.340590           | 0.307263           |\n",
        "| Fasttext            | 0.306072           | 0.292292           |\n",
        "| Fasttext attention  | 0.363475           | 0.316606           |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00015-a67de209-33e5-42d2-b5b5-7454d5a9c0a6",
        "output_cleared": false,
        "id": "ADRg7j13EzzL"
      },
      "source": [
        "## Discussion\n",
        "**BLEU Standard Deviation** - As previously mentioned, the BLEU scores have a very large standard deviation which means that some sentences are very well translated as per the BLEU standard, while some are very wrong.\n",
        "We came up with three interpretations of the cause of this behavior.\n",
        "\n",
        "First, it could be due to an overfitting of our model on the training set, leading to good translations of sentences that are very close to the training set's.\n",
        "\n",
        "Another interpretation is that it could be due to the inadequacy of the BLEU metric for evaluating short sentences.\n",
        "Indeed, it is best used on very long sentences or on whole text corpora but in our case we only made the average of each (relatively short) sentence individually.\n",
        "It thus becomes expected that the standard deviation would be high.\n",
        "Note that the [article by Rachael Tatman](https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213) states that averaging the scores of each sentence in a corpus rather than applying the metric to the full corpus is going to \"artifitially inflate the score\" which is obviously frowned upon by the scientific community.\n",
        "Still, we stick to our averaging for simplicity and it is not too much of a problem as we only compare our results with respect to one another rather than comparing them to others from the literature.\n",
        "\n",
        "Lastly, the dataset we used sometimes has several times the same English sentence with different French translations.\n",
        "Thus, if this English sentence is in both training and validation sets, then the model will be teached to translate this sentence into the French sentence from the training set.\n",
        "However, when evaluating it, the score will expect the validation set French translation and it will receive a bad score because the other French translation is different.\n",
        "This characterizes a well-known limitation of the BLEU score, i.e. that it cannot consider meaning nor sentence structure.\n",
        "Nevertheless, for this specific case, we could have done better and _how_ is explained in the **Improvements** section.\n",
        "\n",
        "**Best Embeddings** - As stated in the results comparison, it seems like the pretrained embeddings performed best and among them, GloVe had the highest average score and the lowest standard deviation.\n",
        "Nevertheless, the deviations are such that this result has only very little reliability.\n",
        "It is known in the literature that FastText is supposedly better than the two others because it is the only one that can handle rare (unknown) words. \n",
        "However, as mentioned previously, we did not use this feature due to the format of the pretrained embeddings and due to the fact that our custom embeddings were never faced with unknown words.\n",
        "Purely in terms of performance, fastText seems worse than the other two in our case but that could either be purely random (since the deviations are so high) or due to a potential inadequate training of the custom embeddings. \n",
        "We cannot emhasize enough that the conclusions we make here are moderately reliable because of the high standard deviations.\n",
        "\n",
        "**Loss and Score** - One very interesting thing we noticed while comparing the loss curves with the scores is that they are strongly correlated.\n",
        "Indeed, when the loss at the 10th epoch of a custom embedding is close to a pretrained embedding's, their scores are close and when the former are far from one another, the scores are as well.\n",
        "This is interesting because the loss is based on the training set while the score is based on the validation set.\n",
        "Consequently, having correlations between the two tends to say that the models can generalize their knowledge of the training set to the validation set.\n",
        "\n",
        "**Time of MT training** - One thing that has grown clearer with each passing hour of work on this project is that the task of Machine Translation and more specifically Neural MT requires a lot of data to train and thus a lot of time.\n",
        "Our full dataset containing 170,000 sentences, we initially thought it would be rather high but it became clear that to train a competitive translator, it would need a lot more than that.\n",
        "Despite that, training on our 170,000 sentences for 10 mere epochs already took about 5 hours at best.\n",
        "As a result, to train a competitive translator, one would need much data but more importantly, a lot more time.\n",
        "\n",
        "**Best Model** - Overviewing our results, we can say that the pretrained embeddings had the best results and that they could probably be even better if given more epochs of training.\n",
        "We also said that it was probably due to the higher dimension of those embeddings.\n",
        "Additionally, we established that the attention mechanism improves greatly the models regardless of the type of embeddings.\n",
        "We can thus hope that the best possible model we could train would be:\n",
        "\n",
        "    A custom embedding model trained on data at the scale of those used for the pretrained embeddings and trained to higher dimensions (300 for instance). It would be trained for more epochs and on a larger dataset of sentences using the attention mechanism.\n",
        "\n",
        "Using custom embeddings even though the best results we obtained were with pretrained embeddings is because using custom embeddings enables the model to exploit the full dataset it learns on (no unknown words and even punctuation).\n",
        "That way we can take the best of both worlds i.e. the higher dimension of the pretrained embeddings and the exhaustiveness (on the dataset) of the custom ones.\n",
        "\n",
        "Strictly limiting ourselves to the models we trained as we trained them, the best one seemed to be **FastText with attention** though it also suffers from having the largest standard deviation of all.\n",
        "Moreover, concluding this way without repeating that the high standard deviations are such that any model (with attention) could have come on top in the end would be careless."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00016-0b1e6cb5-8c4a-4f45-910a-a8ac442a333f",
        "output_cleared": false,
        "id": "6CvXzVnIEzzM"
      },
      "source": [
        "## Improvements\n",
        "- In all the tests we made, the encoder and decoders models were the same, i.e. with a _LSTM_ layer of 1000 units.\n",
        "  This provided a basis to compare the different embeddings. A way to improve the scores obtained would be to **increase the complexity of the encoder and the decoder**, i.e. adding more _LSTM_ layers or more units in a layer. \n",
        "- When using pretrained embeddings, we did not always find French embeddings of **dimensions corresponding** to their English counterpart.\n",
        "For instance, pretrained Word2Vec embeddings had a dimension 300 for English and 200 for French.\n",
        "For the others it's even worse as we did not find pretrained embeddings, we thus used a 100 dimensioned custom french embedding.\n",
        "Somehow finding French embeddings of the same dimension as the English ones would result in more consistency in the networks' learning.\n",
        "It is not a fundamental problem that the dimensions are different but it would be interesting to see the impact it actually has on performance.\n",
        "- As mentioned previously, we chose to train our custom embeddings to a dimension of 100 due to the limitation of the data we had to train it on.\n",
        "To reinforce the reliability of our comparison of pretrained and custom embeddings, it would have been better to **train our custom embeddings on a quantity of data at the same scale as were trained the pretrained** ones.\n",
        "This would have allowed us to confidently train our custom embeddings to a higher dimension as well (e.g. 300 like the pretrained English embeddings) thus hitting two birds with one stone.\n",
        "- Previously, we mentioned that we did not look for a way to **fine-tune the pretrained embeddings on our own dataset** for the sake of comparing custom and pretrained embeddings.\n",
        "However, given more time, it could have been interesting to train additional fine-tuned models to compare them to just-pretrained models and thus establish whether it improves, worsen them or does not change anything.\n",
        "- In terms of **validation**, we established that BLEU does not consider meaning nor sentence structure, is more suited to evaluating corpus as a whole rather than sentences and is better when using more than one reference sentence for each translation evaluation.\n",
        "    The scope of improvements on this matter is thus large.\n",
        "    - We could have simply searched for **other means of evaluating** translations and used them side by side.\n",
        "    - We could have computed BLEU scores while considering **different n-grams** and compared them. (We use the standard n=4)\n",
        "    - We could have computed the **BLEU scores at each epoch** to observe their evolution.\n",
        "    - We could have made a **corpus with our sentences** and computed the BLEU score of the corpus rather than sentence per sentence.\n",
        "    - We could have exploited the repetition of English sentences from our dataset.\n",
        "    Indeed, as mentioned, our dataset contains English sentences that correspond to different French translations.\n",
        "    Consequently, we could gone through our whole dataset and for each sentence present in both the training set and the validation set, we could have added the training French translation to the validation set in order to have an evaluation set containing all of our known possible translations for each English sentence.\n",
        "    We could have thus **evaluated the BLEU score with all our known translations as references** rather than a single one each sentence.\n",
        "\n",
        "- Regarding the **attention mechanism**, we barely scratched the surface of what was possible.\n",
        "    Should we explore this mechanism more in depth, there are a few things we would first try.\n",
        "    - We could explore other attention mechanisms than _Bahdanau_ to compare them e.g. _Luong_'s global and/or local attention mechanisms.\n",
        "    - We could train models where the attention mechanism's context vector is made with the encoder's _cell state_, _hidden state_ and a concatenation of both to compare and discuss their performances.\n",
        "    - We could train the pretrained embedding-based models with the attention as well to see if we can get even higher scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00031-8ee96961-3004-44c6-bde8-69797dc386d3",
        "id": "JZYmwrHCEzzM"
      },
      "source": [
        "## Sources\n",
        "- [TensorFlow tutorial : Neural machine translation with attention](https://www.tensorflow.org/tutorials/text/nmt_with_attention)\n",
        "- [Datasets of bilingual sentence pairs : manythings.org](http://www.manythings.org/anki/)\n",
        "- [Pretrained Embedding vectors : Gensim data](https://github.com/RaRe-Technologies/gensim-data)\n",
        "- [J-P Fauconnier : French word2vec embeddings](https://fauconnier.github.io)\n",
        "- [Medium tutorial : Word vectorization using GloVe](https://medium.com/analytics-vidhya/word-vectorization-using-glove-76919685ee0b)\n",
        "- [Radim Rehurek tutorial : Fasttext](https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html#sphx-glr-auto-examples-tutorials-run-fasttext-py)\n",
        "- [NLTK documentation : _translate.bleu\\_score_](https://www.nltk.org/_modules/nltk/translate/bleu_score.html)\n",
        "- [Towards Data Science article : Evaluating text output in NLP BLEU at your own risk](https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213)\n",
        "- [Towards Data Science article : Sequence 2 sequence model with Attention Mechanism](https://towardsdatascience.com/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00003-cb345316-ed4f-4cf7-bddc-51160f24fe3a",
        "output_cleared": false,
        "id": "XiZfl8HcEzzM"
      },
      "source": [
        "# Part 2 - Implementation\n",
        "1. Data extraction and parsing\n",
        "2. Word embeddings\n",
        "    1. Word2Vec\n",
        "    2. Glove\n",
        "    3. FastText\n",
        "3. Encoder implementation\n",
        "4. Decoder implementation\n",
        "5. Training\n",
        "6. Evaluating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00001-beede662-36f5-418d-ad45-5886f9585e9d",
        "output_cleared": false,
        "id": "iqKcvTrtEzzN"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-nGCJa_SJfV",
        "cell_id": "00001-bc713c7f-7775-48bf-b027-031d8da9a12e",
        "output_cleared": false,
        "source_hash": "44c4b50b",
        "execution_millis": 1667,
        "execution_start": 1605111863752
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from gensim.models.fasttext import FastText\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu as bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import warnings\n",
        "import statistics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00004-f0e99db7-15b6-4eb5-b3be-527e19aebb11",
        "output_cleared": false,
        "id": "tlDEmZRzEzzQ"
      },
      "source": [
        "#### Disable Deprecation Warnings\n",
        "The `gensim` package makes some warnings we do not want to see."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00005-64cf45bb-8baf-44b8-8e2d-8437f04bc6f9",
        "output_cleared": true,
        "source_hash": null,
        "execution_millis": 74,
        "execution_start": 1605175715422,
        "id": "gITo_L67EzzQ"
      },
      "source": [
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00005-a99e4575-8548-4c41-b61e-bf0384ddaba1",
        "output_cleared": false,
        "id": "He56BIYiEzzS"
      },
      "source": [
        "## Constants\n",
        "This section declares all the constants needed to configure this notebook.\n",
        "Note that `num_examples` has been set to `1000`, such that the code can run quickly on a small portion of the dataset. For making a real training on the whole dataset, please set it to `None`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00005-1c2d9a82-efcd-4e31-b1b6-2ca2d587ebc0",
        "output_cleared": false,
        "source_hash": "5e6bbef9",
        "execution_millis": 0,
        "execution_start": 1604857545784,
        "id": "eiR8_0g3EzzT"
      },
      "source": [
        "# General constants\n",
        "drive = False                               # Set to 'True' to store the results on Google Drive\n",
        "restore = False                             # Set to 'True' if the model must be restored\n",
        "train = True                                # Set to 'True' if the model must be trained\n",
        "\n",
        "results_folder = 'results/'                 # Where to store the results\n",
        "checkpoint_dir = 'training_checkpoints/'    # Where to store the weights\n",
        "embeddings_dir = 'custom_embeddings/'       # Where to store the custom embeddings model\n",
        "\n",
        "# Dataset\n",
        "num_examples = 1000                         # Num of sentences taken from the dataset ('None' for all)\n",
        "test_size = 0.1                             # Ratio of sentences used for evaluation\n",
        "split_seed = 42                             # Seed used for splitting the dataset\n",
        "\n",
        "# Embeddings\n",
        "emb_dims = 100                              # Number of dimensions used for the custom embeddings\n",
        "sos_seed = 42                               # Seed used for generating the 'sos' symbol's embeddings\n",
        "eos_seed = 66                               # Seed used for generating the 'eos' symbol's embeddings\n",
        "\n",
        "embed_name_en = 'w2v_custom'                # Embedding model used for the English sentences\n",
        "embed_name_fr = 'w2v_custom'                # Embedding model used for the French sentences\n",
        "# Possible values:\n",
        "# Only English:   w2v_pretrained, glove_pretrained, ft_pretrained\n",
        "# Only French:    w2v_fr_pretrained\n",
        "# Both:           w2v_custom, glove_custom, ft_custom\n",
        "\n",
        "\n",
        "# Encoder - Decoder\n",
        "units = 1000                                # Number of units of the LSTM layer\n",
        "attention = False                           # Set to 'True' for using the attention mechanism\n",
        "\n",
        "# Training\n",
        "batch_size = 64                             # Number of sentences in each batch\n",
        "epochs = 10                                 # Number of epochs\n",
        "\n",
        "# Translating\n",
        "max_length_translation = 100                # Maximum length of a translation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00034-c2825b5f-e9fc-4324-b262-59c6b8fba329",
        "output_cleared": false,
        "source_hash": "9017f6ef",
        "execution_millis": 0,
        "execution_start": 1604857545804,
        "id": "2AzPekWvEzzU"
      },
      "source": [
        "if drive is True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    results_folder = 'drive/My Drive/Colab Notebooks/' + results_folder\n",
        "    checkpoint_dir = 'drive/My Drive/Colab Notebooks/' + checkpoint_dir\n",
        "    embeddings_dir = 'drive/My Drive/Colab Notebooks/' + embeddings_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00003-6f34373c-ef26-4406-9e81-8af491415896",
        "output_cleared": false,
        "id": "k8j7RhiuEzzW"
      },
      "source": [
        "## Data extraction and parsing\n",
        "This section aims at downloading, extracting and preprocessing the sentences of the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00005-407e5c5f-f47d-4eb4-916a-bd97f5fb1a38",
        "output_cleared": false,
        "id": "mW9P6vntEzzW"
      },
      "source": [
        "#### Dataset extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxDXXHmFSY6H",
        "cell_id": "00002-0b800301-e16a-4c4b-8511-233e9c84527f",
        "output_cleared": false,
        "source_hash": "da5fa1f2",
        "execution_millis": 0,
        "execution_start": 1604857545805
      },
      "source": [
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'fra-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip) + \"/fra.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00007-f92a13c3-3160-4004-89ad-82e79fe3acd9",
        "output_cleared": false,
        "id": "hbr82rfqEzzY"
      },
      "source": [
        "#### Dataset parsing\n",
        "All the characters are turned into ascii characters and the sentences are preprocessed as follow:\n",
        "- All the accents are removed,\n",
        "- If we are using a pretrained Word2Vec model, then we only keep the alphabetical characters and \n",
        "  everything else is transformed into a space,\n",
        "- Otherwise, we keep the alphabetical characters as well as the punctuation characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00011-4af0375d-1b57-4eff-8e13-3604634d017b",
        "output_cleared": false,
        "source_hash": "d1fd0a64",
        "execution_millis": 19,
        "execution_start": 1604857545807,
        "id": "6njK4P2IEzzY"
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "    if embed_name_en == \"w2v_pretrained\" or embed_name_fr == \"w2v_fr_pretrained\":\n",
        "        # removing multiple spaces\n",
        "        w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "        # replacing everything with space except (a-z, A-Z)\n",
        "        w = re.sub(r\"[^a-zA-Z]+\", \" \", w)\n",
        "    else:\n",
        "\n",
        "        # adding space between punctuation and characters\n",
        "        w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
        "\n",
        "        # removing multiple spaces\n",
        "        w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "        w = re.sub(r\"[^a-zA-Z?.!]+\", \" \", w)\n",
        "\n",
        "    w = w.strip()\n",
        "\n",
        "    # adding the start and end tokens to the sentence\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00013-b8d409f8-2e14-42ea-ba36-569631383673",
        "output_cleared": false,
        "source_hash": "aeba7e0b",
        "execution_millis": 0,
        "execution_start": 1604857545827,
        "id": "CDSiVpBBEzza"
      },
      "source": [
        "# Return word pairs in the format: [ENGLISH, FRENCH].\n",
        "def create_dataset(path, num_examples):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00040-d1083bd9-3b74-437a-9c41-2c68f9e94f6d",
        "output_cleared": false,
        "id": "h-AUeB_iEzzc"
      },
      "source": [
        "At this point, all the sentences have been preprocessed. But in order to interact with the seq2seq model\n",
        "we have to do two more things:\n",
        "- The encoder and the decoder takes as inputs embeddings, so two embedding models must be created.\n",
        "  This will be done in the next section.\n",
        "- To be able to train the models, the target sentences must be tokenized, i.e. turned into arrays of\n",
        "  numbers, where each number represents a word. This is done next."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00016-22315c4e-1671-4c86-92d3-0729b11e3716",
        "output_cleared": false,
        "source_hash": "176dda57",
        "execution_millis": 0,
        "execution_start": 1604857545828,
        "id": "JCbpbYb-Ezzc"
      },
      "source": [
        "# Tokenize all the sentences contained in lang and pad them.\n",
        "def tokenize(lang):\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "    # tokenize the sentences\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "    # pad the tokenized sentences with '0' so that they always have the same length\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "    # return the tokenized sentences as well as the tokenizer which contains the mapping between the\n",
        "    # words and their related number\n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00014-7b653c00-c52a-441c-ab47-acf86b4cc137",
        "output_cleared": false,
        "source_hash": "9efd4ce3",
        "execution_millis": 1,
        "execution_start": 1604857545828,
        "id": "YUgot90-Ezze"
      },
      "source": [
        "# Pad (non-tokenized) sentences with empty strings so that they always have the same length.\n",
        "def pad_sentences(lang):\n",
        "    max_size = len(max(lang, key=len))\n",
        "\n",
        "    for sentence in lang:\n",
        "        while(len(sentence) < max_size):\n",
        "            sentence.append('')\n",
        "\n",
        "    return lang"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00043-8dfe3b18-0ebc-48f0-b364-d23d7d019eda",
        "output_cleared": false,
        "id": "ybTS1IS2Ezzf"
      },
      "source": [
        "#### Dataset loading\n",
        "Everything related to the dataset has been implemented, it can thus be loaded into some variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00011-16c8ff4b-c7dc-4752-b948-8057f6b825cd",
        "output_cleared": false,
        "source_hash": "59ae9218",
        "execution_millis": 0,
        "execution_start": 1604857545833,
        "id": "nAoqEKLuEzzf"
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "    # load the preprocessed sentences\n",
        "    input_lang, target_lang = create_dataset(path, num_examples)\n",
        "\n",
        "    # tokenize the target sentences and stored them in a tensor\n",
        "    target_tensor, target_lang_tokenizer = tokenize(target_lang)\n",
        "\n",
        "    # split the sentences into list of words\n",
        "    input_lang = [[w for w in s.split()] for s in input_lang]\n",
        "    target_lang = [[w for w in s.split()] for s in target_lang]\n",
        "\n",
        "    # pad the splitted sentences to always have the same length\n",
        "    input_lang = pad_sentences(input_lang)\n",
        "    target_lang = pad_sentences(target_lang)\n",
        "\n",
        "    return input_lang, target_lang, target_tensor, target_lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00012-eca9221b-0a06-49c5-807d-2fecdb8d322f",
        "output_cleared": false,
        "source_hash": "56a7769e",
        "execution_millis": 263,
        "execution_start": 1604857545852,
        "id": "qyF-yWciEzzh"
      },
      "source": [
        "# Load dataset\n",
        "input_lang, target_lang, \\\n",
        "target_tensor, target_lang_tokenizer \\\n",
        "    = load_dataset(path_to_file, num_examples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00013-f19298c8-c8c9-4389-a379-16357e850ef9",
        "output_cleared": false,
        "source_hash": "a9c4dde9",
        "execution_millis": 0,
        "execution_start": 1604857546135,
        "id": "awDUbwLvEzzi"
      },
      "source": [
        "# Creating training and validation sets\n",
        "input_lang_train, input_lang_val, \\\n",
        "target_lang_train, target_lang_val, \\\n",
        "target_tensor_train, target_tensor_val = \\\n",
        "    train_test_split(input_lang, target_lang,target_tensor,\n",
        "                     test_size=test_size, random_state=split_seed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00019-a7c2a69b-1ae3-4ad8-ba27-9762e9930079",
        "output_cleared": false,
        "id": "0p3cYSY8Ezzj"
      },
      "source": [
        "A Tensorflow dataset is created from the preprocessed sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00019-fd8a81fe-0677-4066-9563-0656c9cda4ae",
        "output_cleared": false,
        "source_hash": "5e2e0e0f",
        "execution_millis": 76,
        "execution_start": 1604857546136,
        "id": "PPIKTwRwEzzj"
      },
      "source": [
        "buffer_size = len(input_lang_train)\n",
        "steps_per_epoch = len(input_lang_train) // batch_size\n",
        "vocab_target_size = len(target_lang_tokenizer.word_index) + 1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (input_lang_train, target_lang_train, target_tensor_train)).shuffle(buffer_size)\n",
        "\n",
        "dataset = dataset.batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00015-5d9caad7-7765-48ae-b033-f4a3584af488",
        "output_cleared": false,
        "id": "yVhCXOtvEzzl"
      },
      "source": [
        "\n",
        "## Word Embeddings\n",
        "This section defines several functions that creates the embedding models.\n",
        "There are three big types of embeddings: Word2Vec, GloVe and Fasttext. For each one, it is possible to\n",
        "use a pretrained model or to train a new one on our dataset. In the case of Word2Vec, it is possible to\n",
        "have a pretrained model in both languages, while in GloVe and Fasttext, the pretrained models are only\n",
        "available in English."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00018-848e1ec6-6f72-4037-b569-7da2dca85da9",
        "output_cleared": false,
        "id": "zz6pST_4Ezzl"
      },
      "source": [
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00019-b9131fa7-f125-41af-ab06-f8578aaac236",
        "output_cleared": false,
        "id": "z1TWgFGuEzzm"
      },
      "source": [
        "#### Pre-Trained - English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00018-9c4f70e8-d232-4631-a263-256badcd1f41",
        "output_cleared": false,
        "source_hash": "9b0ea885",
        "execution_millis": 0,
        "execution_start": 1604857546303,
        "id": "iWmeJC5TEzzm"
      },
      "source": [
        "def w2v_pretrained_create(sentences, embed_file):\n",
        "    return api.load('word2vec-google-news-300')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00023-1a2b131d-7151-4aca-9ae7-945ca1fbaffe",
        "output_cleared": false,
        "id": "4QSRgVS0Ezzn"
      },
      "source": [
        "#### Pre-Trained - French"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00024-8e3be67c-1e79-46a0-8aec-60c8e84c535b",
        "output_cleared": false,
        "source_hash": "becbcb3a",
        "execution_millis": 0,
        "execution_start": 1604857546304,
        "id": "u0ekB3iWEzzn"
      },
      "source": [
        "def w2v_fr_pretrained_create(sentences, embed_file):\n",
        "    url = 'http://embeddings.net/embeddings/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin'\n",
        "    path_to_bin = tf.keras.utils.get_file(\n",
        "        'frWac_non_lem_no_postag_no_phrase_200_cbow_cut100',\n",
        "        origin=url,\n",
        "        extract=False)\n",
        "\n",
        "    return KeyedVectors.load_word2vec_format(path_to_bin, binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00016-dfb39153-3a97-4b34-b5e0-76561017fe3a",
        "output_cleared": false,
        "id": "NJdxR5YlEzzp"
      },
      "source": [
        "#### Custom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00019-de70abbe-3c76-4463-ac7e-9de886695e63",
        "output_cleared": false,
        "source_hash": "28c60e8",
        "execution_millis": 1,
        "execution_start": 1604857546305,
        "id": "3KvYDDEGEzzp"
      },
      "source": [
        "def w2v_custom_create(sentences, embed_file):\n",
        "\n",
        "    if os.path.exists(embed_file):\n",
        "        print('Previous adequate custom embeddings found, loading model from file:')\n",
        "        print(embed_file)\n",
        "        return KeyedVectors.load(embed_file)\n",
        "\n",
        "    model = Word2Vec(sentences, size=emb_dims, window=10, min_count=0, workers=4).wv\n",
        "\n",
        "    if not os.path.exists(embeddings_dir):\n",
        "        os.makedirs(embeddings_dir)\n",
        "\n",
        "    model.save(embed_file)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00021-0fae9dd6-4adf-4f94-937b-45eee3b6e910",
        "output_cleared": false,
        "id": "DmYN2yvaEzzq"
      },
      "source": [
        "### GloVe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00045-388a21d2-9f76-463f-8495-80c33c39c08a",
        "output_cleared": false,
        "id": "PNnRuOy-Ezzq"
      },
      "source": [
        "#### Pre-Trained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00028-cd9fda21-d05c-4bff-ac00-449d1f207c52",
        "output_cleared": false,
        "source_hash": "4a9efa14",
        "execution_millis": 1,
        "execution_start": 1604857546306,
        "id": "Ab6NQyMnEzzq"
      },
      "source": [
        "def glove_pretrained_create(sentences, embed_file):\n",
        "        return api.load('glove-wiki-gigaword-300')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00029-e0fc58fb-7ab1-454c-8e39-a711e9df0c38",
        "output_cleared": false,
        "id": "erjt9yzTEzzs"
      },
      "source": [
        "#### Custom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00030-9ec4ac93-4781-4f48-838d-a3a2a103fa8c",
        "output_cleared": false,
        "source_hash": "4c97d93e",
        "execution_millis": 0,
        "execution_start": 1604857546308,
        "id": "t8aDJHq_Ezzs"
      },
      "source": [
        "def glove_custom_create(sentences, embed_file):\n",
        "    !pip install glove_python\n",
        "    from glove import Corpus, Glove\n",
        "\n",
        "    if os.path.exists(embed_file):\n",
        "        print('Previous adequate custom embeddings found, loading model from file:')\n",
        "        print(embed_file)\n",
        "        return Glove.load(embed_file)\n",
        "\n",
        "    corpus = Corpus() \n",
        "    corpus.fit(sentences, window=10)\n",
        "    glove = Glove(no_components=emb_dims, learning_rate=0.05)\n",
        "    \n",
        "    glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=False)\n",
        "\n",
        "    glove.wv = dict()\n",
        "    for w in corpus.dictionary:\n",
        "        glove.wv[w] = glove.word_vectors[corpus.dictionary[w]]\n",
        "\n",
        "    glove.vector_size = emb_dims\n",
        "\n",
        "    if not os.path.exists(embeddings_dir):\n",
        "        os.makedirs(embeddings_dir)\n",
        "\n",
        "    glove.save(embed_file)\n",
        "\n",
        "    return glove"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00024-1dd8a30a-47c5-48b9-8dc2-c05c6ed18f17",
        "output_cleared": false,
        "id": "SG69FI2KEzzt"
      },
      "source": [
        "### FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00028-d8b7e878-a19b-4b64-98f4-5bba2d9319ac",
        "output_cleared": false,
        "id": "n8yKuH4QEzzu"
      },
      "source": [
        "#### Pre-Trained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00029-ae49f60d-e0bc-483b-b61a-622c79ae51e3",
        "output_cleared": false,
        "source_hash": "bca76b56",
        "execution_millis": 20,
        "execution_start": 1604857546309,
        "id": "FecDBJrnEzzu"
      },
      "source": [
        "def ft_pretrained_create(sentences, embed_file):\n",
        "        return api.load('fasttext-wiki-news-subwords-300')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00031-42f4a829-79f4-44a0-98e7-84466a6801e0",
        "output_cleared": false,
        "id": "bF_of1HPEzzv"
      },
      "source": [
        "#### Custom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00025-9ee97501-9dbd-44e1-9d19-fd3be75fec5e",
        "output_cleared": false,
        "source_hash": "ffcd6ca5",
        "execution_millis": 0,
        "execution_start": 1604857546350,
        "id": "C_R2yfi6Ezzv"
      },
      "source": [
        "def ft_custom_create(sentences, embed_file):\n",
        "    if os.path.exists(embed_file):\n",
        "        print('Previous adequate custom embeddings found, loading model from file:')\n",
        "        print(embed_file)\n",
        "        return KeyedVectors.load(embed_file)\n",
        "\n",
        "    model = FastText(sentences, size=emb_dims, window=10, min_count=0, workers=4).wv\n",
        "\n",
        "    if not os.path.exists(embeddings_dir):\n",
        "        os.makedirs(embeddings_dir)\n",
        "\n",
        "    model.save(embed_file)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00030-6762073f-8cf4-4c73-b114-562e99cb4bdb",
        "output_cleared": false,
        "id": "U9Kd_zCTEzzw"
      },
      "source": [
        "Binding the embedding names to the correct creation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00031-82b438ac-9a9c-476d-a4f8-12944ca1fc5f",
        "output_cleared": false,
        "source_hash": "cfea11cf",
        "execution_millis": 19,
        "execution_start": 1604857546351,
        "id": "22CsT8qOEzzw"
      },
      "source": [
        "create_embeddings = dict()\n",
        "\n",
        "create_embeddings['w2v_pretrained'] = w2v_pretrained_create\n",
        "create_embeddings['w2v_fr_pretrained'] = w2v_fr_pretrained_create\n",
        "create_embeddings['w2v_custom'] = w2v_custom_create\n",
        "\n",
        "create_embeddings['glove_pretrained'] = glove_pretrained_create\n",
        "create_embeddings['glove_custom'] = glove_custom_create\n",
        "\n",
        "create_embeddings['ft_pretrained'] = ft_pretrained_create\n",
        "create_embeddings['ft_custom'] = ft_custom_create"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00034-b712c881-481f-49ec-b92e-ddf706c6e923",
        "output_cleared": false,
        "id": "DVUvpiHsEzzx"
      },
      "source": [
        "### Embedding Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00033-67a9ab7e-5c71-4df3-860a-567507bfc90b",
        "output_cleared": false,
        "source_hash": "fc07d4c",
        "execution_millis": 1,
        "execution_start": 1604857546351,
        "id": "jVPU-mzVEzzy"
      },
      "source": [
        "# Return the embeddings of the 'sos' symbol, with the same dimensions as 'model'.\n",
        "def get_sos(model):\n",
        "    return tf.random.stateless_normal(\n",
        "        (model.vector_size,), mean=0.0, stddev=0.00001, seed=(sos_seed, 1))\n",
        "\n",
        "# Return the embeddings of the 'eos' symbol, with the same dimensions as 'model'.\n",
        "def get_eos(model):\n",
        "    return tf.random.stateless_normal(\n",
        "        (model.vector_size,), mean=0.0, stddev=0.00001, seed=(eos_seed, 1))\n",
        "\n",
        "# Return the list of embeddings of a sentence.\n",
        "def embed(sentence, model):\n",
        "    embedded = []\n",
        "    for w in sentence:\n",
        "        # w has been added for padding, its embeddings are all 0's\n",
        "        if w == '':\n",
        "            embedded.append(tf.zeros(model.vector_size))\n",
        "\n",
        "        # w is the sos\n",
        "        elif w == '<start>':\n",
        "            embedded.append(get_sos(model))\n",
        "\n",
        "        # w is the eos\n",
        "        elif w == '<end>':\n",
        "            embedded.append(get_eos(model))\n",
        "\n",
        "        # w is not known by the embedder\n",
        "        elif not w in model.wv:\n",
        "            embedded.append(tf.zeros(model.vector_size))\n",
        "\n",
        "        # normal case where w is a word known by the embedder\n",
        "        else:\n",
        "            embedded.append(model.wv[w])\n",
        "\n",
        "    return embedded\n",
        "\n",
        "# Return the embeddings of a list of sentences.\n",
        "def embed_all_sentences(lang, model):\n",
        "    return tf.convert_to_tensor([embed(s, model) for s in lang])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00033-2e0b61d8-ccb2-46ae-a894-56bb9ba2f2e5",
        "output_cleared": true,
        "source_hash": null,
        "execution_millis": 0,
        "execution_start": 1604857546377,
        "id": "aeNmco16Ezzz"
      },
      "source": [
        "embed_file_en = embeddings_dir + embed_name_en + '_en_' \\\n",
        "              + str(emb_dims) + '_' + str(num_examples) + '.model'\n",
        "\n",
        "embed_file_fr = embeddings_dir + embed_name_fr + '_fr_' \\\n",
        "              + str(emb_dims) + '_' + str(num_examples) + '.model'\n",
        "\n",
        "# Create the chosen embedders\n",
        "embedder_en = create_embeddings[embed_name_en](input_lang, embed_file_en)\n",
        "embedder_fr = create_embeddings[embed_name_fr](target_lang, embed_file_fr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00021-226bcee8-d8d6-4754-a991-18bffcccbc88",
        "output_cleared": false,
        "id": "KV81zJrBEzz0"
      },
      "source": [
        "## Model\n",
        "This section shows how the encoder and the decoder are defined. Note that their architecture remains\n",
        "quite simple, with only one LSTM layer in each model, and a fully connected layer in the decoder to\n",
        "output the word tokens. If the attention mechanism has been enabled, the decoder owns one more layer,\n",
        "the attention one. This attention layer is a custom one, and has been implemented in the tensorflow\n",
        "tutorial from which this notebook is inspired."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00024-1c95cc3e-1e9c-4611-845c-0bd05dc5296c",
        "output_cleared": false,
        "id": "YIobDVRFEzz0"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00039-ae24727d-0367-44d5-bfea-57a18267ba76",
        "output_cleared": false,
        "source_hash": "4798afa6",
        "execution_millis": 0,
        "execution_start": 1604857546378,
        "id": "8fno9T6MEzz0"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.rnn = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                        return_sequences=True,\n",
        "                                        return_state=True,\n",
        "                                        recurrent_initializer='glorot_uniform')        \n",
        "\n",
        "    # Returns the output and the two states of the LSTM layer.\n",
        "    def call(self, x, hidden):\n",
        "        output, hidden_state, cell_state = self.rnn(x, initial_state=hidden)\n",
        "\n",
        "        return output, (hidden_state, cell_state)\n",
        "\n",
        "    # Returns the initial states of the two LSTM layers, i.e. tensors with only 0's. \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00043-1a9dd1cc-8ea3-4fc0-be07-ed8b4b1a3f1b",
        "output_cleared": false,
        "source_hash": "3367680a",
        "execution_millis": 0,
        "execution_start": 1604857546379,
        "id": "DktOtIqLEzz1"
      },
      "source": [
        "encoder = Encoder(units, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00063-edf3819c-1882-4354-862e-11f93ec3e9d3",
        "output_cleared": false,
        "id": "AHzbhZXwEzz2"
      },
      "source": [
        "### Attention Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00064-8d3423bc-2000-4eef-9fc4-2ca2889d63c9",
        "output_cleared": false,
        "source_hash": "df5e358",
        "execution_millis": 0,
        "execution_start": 1604857546380,
        "id": "QtUAdxRqEzz2"
      },
      "source": [
        "if attention is True:\n",
        "    class BahdanauAttention(tf.keras.layers.Layer):\n",
        "        def __init__(self, units):\n",
        "            super(BahdanauAttention, self).__init__()\n",
        "\n",
        "            self.W1 = tf.keras.layers.Dense(units)\n",
        "            self.W2 = tf.keras.layers.Dense(units)\n",
        "\n",
        "            self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "        # Compute the context vector from the current hidden state of the decoder ('query') and\n",
        "        # the outputs of the encoder ('values').\n",
        "        def call(self, query, values):\n",
        "            query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "            score = self.V(tf.nn.tanh(\n",
        "                self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "            attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "            context_vector = attention_weights * values\n",
        "            context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "            return context_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00036-5105cdbd-0db0-452d-afa4-08a3aa429ad0",
        "output_cleared": false,
        "id": "J9anpWWpEzz3"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00037-3721d83d-5264-4590-b24a-71544576ec62",
        "output_cleared": false,
        "source_hash": "a29d712",
        "execution_millis": 1,
        "execution_start": 1604857546380,
        "id": "HPXt8tH3Ezz4"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "\n",
        "        # attention layer\n",
        "        if attention is True:\n",
        "            self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.rnn = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                        return_sequences=True,\n",
        "                                        return_state=True,\n",
        "                                        recurrent_initializer='glorot_uniform')\n",
        "\n",
        "        # FC layer for generating the output word tokens\n",
        "        self.fc = tf.keras.layers.Dense(vocab_target_size)\n",
        "\n",
        "    # Returns the scores of the word tokens and the two states of the LSTM layer.\n",
        "    def call(self, x, hidden, enc_output=None):\n",
        "        if attention:\n",
        "            context_vector = self.attention(hidden[0], enc_output)\n",
        "\n",
        "            x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        output, hidden_state, cell_state = self.rnn(x, initial_state=hidden)\n",
        "\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, (hidden_state, cell_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00041-6bea1384-3d68-4df4-a0a9-4c1400a24b9c",
        "output_cleared": false,
        "source_hash": "ebdc0817",
        "execution_millis": 0,
        "execution_start": 1604857546382,
        "id": "bkQydy_0Ezz4"
      },
      "source": [
        "decoder = Decoder(units, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00042-a3635b15-534a-4fd1-82db-93cae3d09a84",
        "output_cleared": false,
        "id": "ev3UDW9gEzz6"
      },
      "source": [
        "## Optimizer and Loss\n",
        "There is one thing left to do before the training: choosing the optimizer and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00043-6f160538-9bf1-4d92-9bb4-72e2e781da45",
        "output_cleared": false,
        "source_hash": "bcafcd07",
        "execution_millis": 0,
        "execution_start": 1604857546383,
        "id": "mzG_nbeuEzz6"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "# Returns cross-entropy loss while applying the padding mask. \n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00043-a7272ef0-834b-42ef-a95f-371fd7bfaad7",
        "output_cleared": false,
        "id": "pBNOdkBaEzz7"
      },
      "source": [
        "## Training\n",
        "This section defines two things:\n",
        "- A train step which computes the loss on a batch and updates the weights of the models. It is\n",
        "  implemented as a `tf.function` to speed up training.\n",
        "- A train loop that iterates a certain number of epochs over the whole training dataset and that calls\n",
        "  the train step for each batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00044-0844a8fe-9cf5-44aa-a562-ddaccdfe616b",
        "output_cleared": false,
        "source_hash": "2883b261",
        "execution_millis": 2,
        "execution_start": 1604857546384,
        "id": "h6VDSA3WEzz7"
      },
      "source": [
        "# Computes the loss over a batch, updates the models and returns the loss\n",
        "@tf.function\n",
        "def train_step(input_lang, target_lang, target_tensor, enc_hidden):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(input_lang, enc_hidden)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        for t in range(target_lang.shape[1] - 1):\n",
        "            dec_input = tf.expand_dims(target_lang[:, t], axis=1)\n",
        "\n",
        "            predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "            loss += loss_function(target_tensor[:, t+1], predictions)\n",
        "\n",
        "    batch_loss = (loss / int(target_lang.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00047-2ed27f03-2b19-4ced-be98-d0c06bd24982",
        "output_cleared": false,
        "source_hash": "b21f20ad",
        "execution_millis": 0,
        "execution_start": 1604857546387,
        "id": "cU1LkdzJEzz8"
      },
      "source": [
        "# Creates a tensorflow Checkpoint to save the weights\n",
        "ckpt_dir = checkpoint_dir + embed_name_en + ('_with_attention' if attention is True else '')\n",
        "checkpoint_prefix = os.path.join(ckpt_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00087-f81cbc54-165d-4142-ae19-c3a85d3682cd",
        "output_cleared": false,
        "source_hash": "295efe9b",
        "execution_millis": 21,
        "execution_start": 1604857546395,
        "id": "gcHpwqhuEzz9"
      },
      "source": [
        "# Restore a previous checkpoint\n",
        "if restore is True:\n",
        "    if os.path.exists(ckpt_dir) and os.listdir(ckpt_dir):\n",
        "        checkpoint.restore(tf.train.latest_checkpoint(ckpt_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00059-e56485d1-577d-4bef-a75a-277225ed8b9e",
        "output_cleared": false,
        "source_hash": "5f5fee32",
        "execution_millis": 1,
        "execution_start": 1604857546416,
        "id": "0O78JNJeEzz-"
      },
      "source": [
        "# Transform a numpy array of bites string into a list of lists of words (= list of sentences)\n",
        "def to_list_strings(a):\n",
        "    sentences = np.empty(a.shape, dtype=object)\n",
        "\n",
        "    for (x, y), w in np.ndenumerate(a):\n",
        "        sentences[x, y] = w.decode('utf-8')\n",
        "    \n",
        "    return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00059-86ba34ca-799c-4225-a77b-588e71885731",
        "output_cleared": true,
        "source_hash": null,
        "execution_millis": 17827,
        "execution_start": 1604857546422,
        "id": "aCeqyX45Ezz_"
      },
      "source": [
        "if train is True:\n",
        "    # creates the results folder if required\n",
        "    if not os.path.exists(results_folder):\n",
        "        os.makedirs(results_folder)\n",
        "\n",
        "    filename = results_folder + embed_name_en \\\n",
        "            + ('_with_attention' if attention is True else '') + '_loss.csv'\n",
        "\n",
        "    # creates the results file. If restoration is enabled and the file already exists, then we do\n",
        "    # not recreate it, and we will append our results to it.\n",
        "    if restore is True:\n",
        "        if not os.path.exists(filename):\n",
        "            with open(filename, 'w') as f:\n",
        "                f.write('# mean,stddev\\n')\n",
        "\n",
        "    else:\n",
        "        with open(filename, 'w') as f:\n",
        "                f.write('# mean,stddev\\n')\n",
        "\n",
        "    results = np.zeros((2, 2))\n",
        "\n",
        "    # training loop\n",
        "    for epoch in range(epochs):\n",
        "        epoch_losses = []\n",
        "        start = time.time()\n",
        "\n",
        "        enc_hidden = encoder.initialize_hidden_state()\n",
        "        total_loss = 0\n",
        "\n",
        "        for (batch, (input_lang, target_lang, target_tensor)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "\n",
        "            # transform input_lang and target_lang into list of list of words\n",
        "            input_lang = to_list_strings(input_lang.numpy())\n",
        "            target_lang = to_list_strings(target_lang.numpy())\n",
        "\n",
        "            # computes the embeddings\n",
        "            input_embed = embed_all_sentences(input_lang, embedder_en)\n",
        "            target_embed = embed_all_sentences(target_lang, embedder_fr)\n",
        "\n",
        "            # computes the loss and updates the models\n",
        "            batch_loss = train_step(input_embed, target_embed, target_tensor, enc_hidden)\n",
        "            epoch_losses.append(batch_loss)\n",
        "            total_loss += batch_loss\n",
        "\n",
        "            if batch % 100 == 0:\n",
        "                print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
        "\n",
        "        epoch_losses = np.array(epoch_losses)\n",
        "        results[epoch%2, 0] = epoch_losses.mean()\n",
        "        results[epoch%2, 1] = epoch_losses.std()\n",
        "\n",
        "        # saves models and results every 2 epochs\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "            with open(filename, 'a') as f:\n",
        "                np.savetxt(f, results, delimiter=',')\n",
        "\n",
        "        print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                            total_loss / steps_per_epoch))\n",
        "        print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00053-bfc9def0-059b-4ad1-8b16-b575566d7f21",
        "output_cleared": false,
        "id": "-yAKasAXEz0A"
      },
      "source": [
        "## Translating\n",
        "This section defines the functions required for translating a sentence with the trained model,\n",
        "and thus evaluating it. Note that the translation is generated by taking at each time the most probable\n",
        "word. This could be improved by using beam search for example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00054-74e1c834-b6ef-44bd-8b9b-66073cfe977e",
        "output_cleared": false,
        "source_hash": "c28d6a59",
        "execution_millis": 2,
        "execution_start": 1604857564269,
        "id": "kZ9ygmseEz0A"
      },
      "source": [
        "# Returns the translated sentence given the embeddings of the input sequence.\n",
        "def translate_embed(embeddings):\n",
        "    # initial state of the encoder\n",
        "    hidden = tf.zeros((1, units)), tf.zeros((1, units))\n",
        "\n",
        "    # pass through the encoder\n",
        "    enc_output, enc_hidden = encoder(tf.expand_dims(embeddings, axis=0), hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    l = 0\n",
        "    word_input_embed = get_sos(embedder_fr)\n",
        "    translation = []\n",
        "    \n",
        "    # loop for generating the translation\n",
        "    while l < max_length_translation:\n",
        "        # pass through the decoder\n",
        "        dec_output, dec_hidden = decoder(\n",
        "            tf.reshape(word_input_embed, (1, 1, word_input_embed.shape[0])), dec_hidden, enc_output)\n",
        "            \n",
        "        dec_output = tf.reshape(dec_output, (dec_output.shape[1]))\n",
        "\n",
        "        # get the most probable token\n",
        "        token_output = tf.math.argmax(dec_output).numpy()\n",
        "\n",
        "        # get the word associated to this token\n",
        "        if token_output == 0:\n",
        "            word_output = '<unk>'\n",
        "\n",
        "        else:\n",
        "            word_output = target_lang_tokenizer.index_word[token_output]\n",
        "\n",
        "        # stop the translating process if the decoder has outputed an eos\n",
        "        if word_output == '<end>':\n",
        "            return translation\n",
        "\n",
        "        translation.append(word_output)\n",
        "\n",
        "        # get the embeddings of the outputed word, which will be the next inputs of the decoder\n",
        "        word_input_embed = tf.convert_to_tensor(embed([word_output], embedder_fr)[0])\n",
        "\n",
        "        l += 1\n",
        "\n",
        "    return translation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00054-644c4db6-1987-499f-856c-c107010d9e06",
        "output_cleared": false,
        "source_hash": "226a42f3",
        "execution_millis": 4,
        "execution_start": 1604857564271,
        "id": "eBbPBhxDEz0B"
      },
      "source": [
        "# Translate a sentence using the trained model.\n",
        "def translate(sentence):\n",
        "    # preprocess sentence and split it into words\n",
        "    sentence = preprocess_sentence(sentence).split()\n",
        "\n",
        "    # get the embeddings\n",
        "    embeddings = tf.convert_to_tensor(embed(sentence, embedder_en))\n",
        "\n",
        "    # translate\n",
        "    return translate_embed(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00055-0fe29056-f31c-4dc3-8fda-2d1049c56aaf",
        "output_cleared": false,
        "source_hash": "8fa13187",
        "execution_millis": 36,
        "execution_start": 1604857564275,
        "id": "YjNptXbhEz0C"
      },
      "source": [
        "' '.join(translate('hi'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00059-f34138a7-ca2c-428e-9222-12693ea4e67f",
        "output_cleared": false,
        "id": "pYm07TDSEz0E"
      },
      "source": [
        "## Evaluating\n",
        "The last part of this notebook consists of an evaluating function using the BLEU score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00060-377a73d3-408f-4acd-8c98-038dcc35125a",
        "output_cleared": false,
        "source_hash": "508d6e37",
        "execution_millis": 1,
        "execution_start": 1604857564372,
        "id": "jU2vj-CFEz0E"
      },
      "source": [
        "# Returns the mean and the standard deviation of the BLEU scores obtained.\n",
        "def evaluate(input_embed, target_lang):\n",
        "    translations = []\n",
        "    scores = []\n",
        "    # Method 1 was chosen as it seemed the simplest, it simply adds a fixed very small value (epsilon) to\n",
        "    # the numerator of each precision score when it is zero.\n",
        "    # Source: https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n",
        "    sf = SmoothingFunction().method1\n",
        "    for index, e in enumerate(input_embed):\n",
        "        translation = translate_embed(e)\n",
        "        trunc_target = target_lang[index][1:(target_lang[index].index(\"<end>\"))]\n",
        "\n",
        "        # computing BLEU score\n",
        "        if (len(trunc_target) > 3 and len(translation) > 3):\n",
        "            scores.append(bleu([trunc_target], translation, smoothing_function=sf))\n",
        "        else:\n",
        "            # Special case when the sentences have less than 4 words\n",
        "            shortest = min(len(trunc_target), len(translation))\n",
        "            weights = tuple()\n",
        "            for _ in range(shortest):\n",
        "                weights += (1./shortest,)\n",
        "\n",
        "            scores.append(bleu([trunc_target], translation, smoothing_function=sf, weights=weights))\n",
        "\n",
        "    scores = np.array(scores)\n",
        "    return scores.mean(), scores.std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00061-4f1cc6d0-badb-403e-809a-15a08a63eacb",
        "output_cleared": true,
        "source_hash": null,
        "execution_millis": 1930,
        "execution_start": 1604857564374,
        "id": "wW9uMBHvEz0F"
      },
      "source": [
        "input_embed_val = embed_all_sentences(input_lang_val, embedder_en)\n",
        "mean, std = evaluate(input_embed_val, target_lang_val)\n",
        "\n",
        "print('BLEU score: mean = {}, std = {}'.format(mean, std))\n",
        "\n",
        "with open(results_folder + embed_name_en + ('_with_attention' if attention is True else '') + '_bleu.csv', 'w') as file:\n",
        "    file.write('# mean,stddev\\n')\n",
        "    file.write(str(mean) + ',' + str(std) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00096-f310898d-942f-4387-8035-e23050cde432",
        "output_cleared": false,
        "source_hash": "b623e53d",
        "execution_millis": 0,
        "execution_start": 1604857566305,
        "id": "SfR9PqjcEz0G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}